[GLOBALS]
# loss can be either cross_entropy or MSE
loss: cross_entropy
batch_size: 16
epochs: 2

# 0 | 1 | 2
verbose: 1

# weight regularization
wreg: 0.1
# wrt: L1 | L2 | none
wrt: L2


# layer activation function available options: sigmoid | tanh | relu | linear
[LAYERS]
# input format: type - size
i: conv - none

# dense format: 'dense' - size - activation - learning rate
# conv format: 'conv' - kernel shape - number of kernels - stride - mode - activation - learning rate
1: conv - (3,3) - 3 - 1 - same - tanh - 0.01
2: conv - (3,3) - 3 - 1 - valid - tanh - 0.01
3: dense - 100 - relu - 0.01
# output format: size - activation - learning rate
o: 4 - softmax - 0.001



#1: conv - (3,3) - 2 - 3 - same - relu - 0.06
#2: dense - 100 - relu - 0.01
#3: conv - (1,3) - 3 - 2 - full - relu - 0.01
#4: dense - 100 - relu - 0.01
#5: dense - 20 - linear - 0.05
# output format: size - activation - learning rate
#o: 4 - softmax - 0.001


[DATA]
image_size: 16
two_dimensional: true
centered: true
noise_rate: 0.05
number_of_images: 2000
flatten: false
# share test = 1 - train - validate
share_train: 0.8
share_validate: 0.05